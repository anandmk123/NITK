{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PnZPsRgIcm5A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('PAT_Dataset.csv')\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = shuffle(df, random_state=42)\n",
        "\n",
        "# Columns containing the target variables\n",
        "target_columns = [\n",
        "    'steady_state_temp_L0', 'steady_state_temp_L1', 'router_avg_temp_L0', 'router_avg_temp_L1',\n",
        "    'core_avg_temp_L0', 'core_avg_temp_L1', 'mem_avg_temp_L0', 'mem_avg_temp_L1',\n",
        "    'total_area', 'avg_power', 'avg_cores_power', 'avg_routers_power', 'avg_power_per_router',\n",
        "    'layer_area', 'area_per_core'\n",
        "]\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_columns = ['routing_type', 'selection_strategy', 'traffic_type']\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "X = pd.get_dummies(df.drop(columns=target_columns), columns=categorical_columns, drop_first=True)\n",
        "y = df[target_columns]\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Scale the feature and target data\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Reshape the data to fit a CNN input\n",
        "# The CNN model expects a 3D input shape (samples, time steps, features), so we reshape X_train_scaled and X_test_scaled\n",
        "X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)  # Add 1 for the single \"channel\"\n",
        "X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Build a simple CNN model\n",
        "cnn_model = Sequential()\n",
        "\n",
        "# Add 1D convolutional layers with padding='same' to avoid shrinking the input size\n",
        "cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Flatten the output of the convolutions\n",
        "cnn_model.add(Flatten())\n",
        "\n",
        "# Add dense layers for regression\n",
        "cnn_model.add(Dense(128, activation='relu'))\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dense(len(target_columns)))  # Output layer for all targets at once\n",
        "\n",
        "# Compile the CNN model\n",
        "cnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train_cnn, y_train_scaled, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict using the trained CNN model\n",
        "y_pred_cnn_scaled = cnn_model.predict(X_test_cnn)\n",
        "\n",
        "# Prepare a list to store the evaluation results\n",
        "cnn_evaluation_results = []\n",
        "\n",
        "# Evaluate the model for each target variable separately\n",
        "print(\"CNN Performance (per target variable):\")\n",
        "for i, target in enumerate(target_columns):\n",
        "    y_test_target = y_test_scaled[:, i]  # Get actual values for target i\n",
        "    y_pred_target = y_pred_cnn_scaled[:, i]  # Get predicted values for target i\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(y_test_target, y_pred_target)\n",
        "    mae = mean_absolute_error(y_test_target, y_pred_target)\n",
        "    r2 = r2_score(y_test_target, y_pred_target)\n",
        "\n",
        "    # Append results to the list\n",
        "    cnn_evaluation_results.append({\n",
        "        'algorithm': 'CNN',\n",
        "        'target': target,\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'R²': r2\n",
        "    })\n",
        "\n",
        "    # Print results for the current target\n",
        "    print(f\"\\nTarget: {target}\")\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R²:\", r2)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Convert the results list to a DataFrame\n",
        "cnn_evaluation_df = pd.DataFrame(cnn_evaluation_results)\n",
        "\n",
        "# Display the CNN evaluation results DataFrame\n",
        "print(\"\\nCNN Evaluation Results DataFrame:\")\n",
        "print(cnn_evaluation_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21uOq-JRRV0b",
        "outputId": "d5b446f5-b403-49d0-8cc8-f0af5b7fc483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.4400 - val_loss: 0.1242\n",
            "Epoch 2/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1163 - val_loss: 0.0816\n",
            "Epoch 3/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0808 - val_loss: 0.0680\n",
            "Epoch 4/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0600 - val_loss: 0.0433\n",
            "Epoch 5/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0419 - val_loss: 0.0337\n",
            "Epoch 6/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0326 - val_loss: 0.0260\n",
            "Epoch 7/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0236 - val_loss: 0.0167\n",
            "Epoch 8/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0161 - val_loss: 0.0149\n",
            "Epoch 9/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0137 - val_loss: 0.0129\n",
            "Epoch 10/10\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0142 - val_loss: 0.0102\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "CNN Performance (per target variable):\n",
            "\n",
            "Target: steady_state_temp_L0\n",
            "MSE: 0.02096937110308532\n",
            "MAE: 0.08261156588842983\n",
            "R²: 0.9781676955569012\n",
            "------------------------------\n",
            "\n",
            "Target: steady_state_temp_L1\n",
            "MSE: 0.020049396778810943\n",
            "MAE: 0.07894665584945136\n",
            "R²: 0.9791335486174971\n",
            "------------------------------\n",
            "\n",
            "Target: router_avg_temp_L0\n",
            "MSE: 0.016368692828674522\n",
            "MAE: 0.08571156224689672\n",
            "R²: 0.9835818883826399\n",
            "------------------------------\n",
            "\n",
            "Target: router_avg_temp_L1\n",
            "MSE: 0.011438306651250988\n",
            "MAE: 0.06487793952148743\n",
            "R²: 0.9890909323629433\n",
            "------------------------------\n",
            "\n",
            "Target: core_avg_temp_L0\n",
            "MSE: 0.019988799019686233\n",
            "MAE: 0.09852971499943562\n",
            "R²: 0.9797093486489623\n",
            "------------------------------\n",
            "\n",
            "Target: core_avg_temp_L1\n",
            "MSE: 0.011870291127710835\n",
            "MAE: 0.07088043761091199\n",
            "R²: 0.9882729838454228\n",
            "------------------------------\n",
            "\n",
            "Target: mem_avg_temp_L0\n",
            "MSE: 0.023863576063990338\n",
            "MAE: 0.10737464005255769\n",
            "R²: 0.976250133818812\n",
            "------------------------------\n",
            "\n",
            "Target: mem_avg_temp_L1\n",
            "MSE: 0.011545531826022946\n",
            "MAE: 0.06548455419603429\n",
            "R²: 0.9887678373072305\n",
            "------------------------------\n",
            "\n",
            "Target: total_area\n",
            "MSE: 0.001406794509139666\n",
            "MAE: 0.029944346964333887\n",
            "R²: 0.9986242580045358\n",
            "------------------------------\n",
            "\n",
            "Target: avg_power\n",
            "MSE: 0.0016974801501843464\n",
            "MAE: 0.030097717974122765\n",
            "R²: 0.9983011576398029\n",
            "------------------------------\n",
            "\n",
            "Target: avg_cores_power\n",
            "MSE: 0.001330102212230966\n",
            "MAE: 0.028484001763148314\n",
            "R²: 0.9986782513342043\n",
            "------------------------------\n",
            "\n",
            "Target: avg_routers_power\n",
            "MSE: 0.0032203148335788073\n",
            "MAE: 0.03893102527267633\n",
            "R²: 0.996703912813066\n",
            "------------------------------\n",
            "\n",
            "Target: avg_power_per_router\n",
            "MSE: 0.01329662734445782\n",
            "MAE: 0.08820980961569752\n",
            "R²: 0.9864068144074941\n",
            "------------------------------\n",
            "\n",
            "Target: layer_area\n",
            "MSE: 0.001159955456948874\n",
            "MAE: 0.02685491314204353\n",
            "R²: 0.9988656482450251\n",
            "------------------------------\n",
            "\n",
            "Target: area_per_core\n",
            "MSE: 0.00024272287865772114\n",
            "MAE: 0.01153039067196583\n",
            "R²: 0.0\n",
            "------------------------------\n",
            "\n",
            "CNN Evaluation Results DataFrame:\n",
            "   algorithm                target       MSE       MAE        R²\n",
            "0        CNN  steady_state_temp_L0  0.020969  0.082612  0.978168\n",
            "1        CNN  steady_state_temp_L1  0.020049  0.078947  0.979134\n",
            "2        CNN    router_avg_temp_L0  0.016369  0.085712  0.983582\n",
            "3        CNN    router_avg_temp_L1  0.011438  0.064878  0.989091\n",
            "4        CNN      core_avg_temp_L0  0.019989  0.098530  0.979709\n",
            "5        CNN      core_avg_temp_L1  0.011870  0.070880  0.988273\n",
            "6        CNN       mem_avg_temp_L0  0.023864  0.107375  0.976250\n",
            "7        CNN       mem_avg_temp_L1  0.011546  0.065485  0.988768\n",
            "8        CNN            total_area  0.001407  0.029944  0.998624\n",
            "9        CNN             avg_power  0.001697  0.030098  0.998301\n",
            "10       CNN       avg_cores_power  0.001330  0.028484  0.998678\n",
            "11       CNN     avg_routers_power  0.003220  0.038931  0.996704\n",
            "12       CNN  avg_power_per_router  0.013297  0.088210  0.986407\n",
            "13       CNN            layer_area  0.001160  0.026855  0.998866\n",
            "14       CNN         area_per_core  0.000243  0.011530  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the new data as a DataFrame\n",
        "new_data = pd.DataFrame({\n",
        "    'dimx': [14],\n",
        "    'dimy': [11],\n",
        "    'dimz': [2],\n",
        "    'buffer_size': [10],\n",
        "    'packet_size_min': [4],\n",
        "    'packet_size_max': [8],\n",
        "    'routing_type_oe_3d': [1],  # Make sure to align with the encoded columns\n",
        "    'selection_strategy_thermal': [1],\n",
        "    'traffic_type_random': [1],\n",
        "    'injection_rate': [0.05]\n",
        "})\n",
        "\n",
        "# Ensure all columns are present (including any missing dummy variables from the training data)\n",
        "# Missing columns in new_data are filled with 0s\n",
        "for col in X.columns:\n",
        "    if col not in new_data:\n",
        "        new_data[col] = 0\n",
        "\n",
        "# Reorder the new_data columns to match the original feature order\n",
        "new_data = new_data[X.columns]\n",
        "\n",
        "# Scale the new data using the previously fitted scaler\n",
        "new_data_scaled = scaler_X.transform(new_data)\n",
        "\n",
        "# Reshape the scaled data to fit the CNN input shape\n",
        "new_data_cnn = new_data_scaled.reshape(1, new_data_scaled.shape[1], 1)  # Shape (1, num_features, 1)\n",
        "\n",
        "# Make predictions with the CNN model\n",
        "new_data_pred_scaled = cnn_model.predict(new_data_cnn)\n",
        "\n",
        "# Inverse scale the predictions to get original scale\n",
        "new_data_pred = scaler_y.inverse_transform(new_data_pred_scaled)\n",
        "\n",
        "# Convert the prediction to a DataFrame for readability\n",
        "predictions_df = pd.DataFrame(new_data_pred, columns=target_columns)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions for the new data:\")\n",
        "print(predictions_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSzYt6hVPql_",
        "outputId": "7c711c79-c240-48fe-e33e-a9eb6bc22ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "Predictions for the new data:\n",
            "   steady_state_temp_L0  steady_state_temp_L1  router_avg_temp_L0  \\\n",
            "0            143.108994            141.062057           26.920168   \n",
            "\n",
            "   router_avg_temp_L1  core_avg_temp_L0  core_avg_temp_L1  mem_avg_temp_L0  \\\n",
            "0           26.680582         26.157522         26.034374        25.874044   \n",
            "\n",
            "   mem_avg_temp_L1   total_area     avg_power  avg_cores_power  \\\n",
            "0         25.76001  718245888.0  4.409188e-07     3.453331e-07   \n",
            "\n",
            "   avg_routers_power  avg_power_per_router   layer_area  area_per_core  \n",
            "0       8.806025e-08          2.926545e-10  368154816.0      4695230.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Build an improved CNN model with dropout layers and deeper architecture\n",
        "cnn_model = Sequential()\n",
        "\n",
        "cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "cnn_model.add(Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'))\n",
        "cnn_model.add(MaxPooling1D(pool_size=2))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "\n",
        "# Flatten and add dense layers for regression\n",
        "cnn_model.add(Flatten())\n",
        "cnn_model.add(Dense(128, activation='relu'))\n",
        "cnn_model.add(Dropout(0.3))\n",
        "cnn_model.add(Dense(64, activation='relu'))\n",
        "cnn_model.add(Dense(len(target_columns)))  # Output layer for all targets at once\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Set up callbacks for early stopping and learning rate reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train_cnn, y_train_scaled, epochs=50, batch_size=32, validation_split=0.2,\n",
        "              callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Predict using the trained CNN model\n",
        "y_pred_cnn_scaled = cnn_model.predict(X_test_cnn)\n",
        "\n",
        "# Prepare evaluation results as before\n",
        "cnn_evaluation_results = []\n",
        "print(\"CNN Performance (per target variable):\")\n",
        "\n",
        "for i, target in enumerate(target_columns):\n",
        "    y_test_target = y_test_scaled[:, i]\n",
        "    y_pred_target = y_pred_cnn_scaled[:, i]\n",
        "    mse = mean_squared_error(y_test_target, y_pred_target)\n",
        "    mae = mean_absolute_error(y_test_target, y_pred_target)\n",
        "    r2 = r2_score(y_test_target, y_pred_target)\n",
        "    cnn_evaluation_results.append({\n",
        "        'algorithm': 'CNN',\n",
        "        'target': target,\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'R²': r2\n",
        "    })\n",
        "    print(f\"\\nTarget: {target}\")\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R²:\", r2)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Results DataFrame\n",
        "cnn_evaluation_df = pd.DataFrame(cnn_evaluation_results)\n",
        "print(\"\\nCNN Evaluation Results DataFrame:\")\n",
        "print(cnn_evaluation_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKPohrkDVDLe",
        "outputId": "6f02d03e-c9f3-4404-90ce-9f9dc084b015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.5703 - val_loss: 0.1989 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2484 - val_loss: 0.1069 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1773 - val_loss: 0.0833 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1434 - val_loss: 0.0801 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1329 - val_loss: 0.0571 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1214 - val_loss: 0.0584 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1135 - val_loss: 0.0616 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1039 - val_loss: 0.0426 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0964 - val_loss: 0.0446 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0960 - val_loss: 0.0402 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0892 - val_loss: 0.0347 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0811 - val_loss: 0.0302 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0803 - val_loss: 0.0293 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0756 - val_loss: 0.0359 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0789 - val_loss: 0.0322 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0726 - val_loss: 0.0235 - learning_rate: 0.0010\n",
            "Epoch 17/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0694 - val_loss: 0.0299 - learning_rate: 0.0010\n",
            "Epoch 18/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0702 - val_loss: 0.0285 - learning_rate: 0.0010\n",
            "Epoch 19/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0713 - val_loss: 0.0280 - learning_rate: 0.0010\n",
            "Epoch 20/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0553 - val_loss: 0.0172 - learning_rate: 2.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0503 - val_loss: 0.0170 - learning_rate: 2.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0517 - val_loss: 0.0176 - learning_rate: 2.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0492 - val_loss: 0.0185 - learning_rate: 2.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0493 - val_loss: 0.0173 - learning_rate: 2.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0487 - val_loss: 0.0167 - learning_rate: 4.0000e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0473 - val_loss: 0.0164 - learning_rate: 4.0000e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0462 - val_loss: 0.0154 - learning_rate: 4.0000e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0466 - val_loss: 0.0161 - learning_rate: 4.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0485 - val_loss: 0.0155 - learning_rate: 4.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0457 - val_loss: 0.0157 - learning_rate: 4.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0464 - val_loss: 0.0157 - learning_rate: 1.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0455 - val_loss: 0.0158 - learning_rate: 1.0000e-05\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "CNN Performance (per target variable):\n",
            "\n",
            "Target: steady_state_temp_L0\n",
            "MSE: 0.02524318472379047\n",
            "MAE: 0.09745777152609301\n",
            "R²: 0.9737180055952138\n",
            "------------------------------\n",
            "\n",
            "Target: steady_state_temp_L1\n",
            "MSE: 0.025388434876355462\n",
            "MAE: 0.09808902270475205\n",
            "R²: 0.9735769336170158\n",
            "------------------------------\n",
            "\n",
            "Target: router_avg_temp_L0\n",
            "MSE: 0.02331115738701918\n",
            "MAE: 0.09055735560188723\n",
            "R²: 0.9766184638006357\n",
            "------------------------------\n",
            "\n",
            "Target: router_avg_temp_L1\n",
            "MSE: 0.01557867988039346\n",
            "MAE: 0.06882839962591349\n",
            "R²: 0.9851421300640967\n",
            "------------------------------\n",
            "\n",
            "Target: core_avg_temp_L0\n",
            "MSE: 0.02763259159427248\n",
            "MAE: 0.10963220745732039\n",
            "R²: 0.9719501265977609\n",
            "------------------------------\n",
            "\n",
            "Target: core_avg_temp_L1\n",
            "MSE: 0.021284793968151406\n",
            "MAE: 0.08975718125351562\n",
            "R²: 0.9789721144977938\n",
            "------------------------------\n",
            "\n",
            "Target: mem_avg_temp_L0\n",
            "MSE: 0.03487144512571244\n",
            "MAE: 0.11975766429836573\n",
            "R²: 0.96529471722681\n",
            "------------------------------\n",
            "\n",
            "Target: mem_avg_temp_L1\n",
            "MSE: 0.019534599745447512\n",
            "MAE: 0.08044440473579446\n",
            "R²: 0.9809956088826978\n",
            "------------------------------\n",
            "\n",
            "Target: total_area\n",
            "MSE: 0.004349301639089114\n",
            "MAE: 0.05043776888999041\n",
            "R²: 0.995746701542434\n",
            "------------------------------\n",
            "\n",
            "Target: avg_power\n",
            "MSE: 0.004476854875289679\n",
            "MAE: 0.04986262193559363\n",
            "R²: 0.9955195524956381\n",
            "------------------------------\n",
            "\n",
            "Target: avg_cores_power\n",
            "MSE: 0.004272910402279757\n",
            "MAE: 0.049713276498365065\n",
            "R²: 0.9957539250958729\n",
            "------------------------------\n",
            "\n",
            "Target: avg_routers_power\n",
            "MSE: 0.006572783032776472\n",
            "MAE: 0.05497332425111954\n",
            "R²: 0.993272562759724\n",
            "------------------------------\n",
            "\n",
            "Target: avg_power_per_router\n",
            "MSE: 0.01385876004392065\n",
            "MAE: 0.07946821760032936\n",
            "R²: 0.985832144311577\n",
            "------------------------------\n",
            "\n",
            "Target: layer_area\n",
            "MSE: 0.004225792488209651\n",
            "MAE: 0.049324989486230625\n",
            "R²: 0.9958674834482273\n",
            "------------------------------\n",
            "\n",
            "Target: area_per_core\n",
            "MSE: 7.73666184801724e-09\n",
            "MAE: 5.596590977210178e-05\n",
            "R²: 0.0\n",
            "------------------------------\n",
            "\n",
            "CNN Evaluation Results DataFrame:\n",
            "   algorithm                target           MSE       MAE        R²\n",
            "0        CNN  steady_state_temp_L0  2.524318e-02  0.097458  0.973718\n",
            "1        CNN  steady_state_temp_L1  2.538843e-02  0.098089  0.973577\n",
            "2        CNN    router_avg_temp_L0  2.331116e-02  0.090557  0.976618\n",
            "3        CNN    router_avg_temp_L1  1.557868e-02  0.068828  0.985142\n",
            "4        CNN      core_avg_temp_L0  2.763259e-02  0.109632  0.971950\n",
            "5        CNN      core_avg_temp_L1  2.128479e-02  0.089757  0.978972\n",
            "6        CNN       mem_avg_temp_L0  3.487145e-02  0.119758  0.965295\n",
            "7        CNN       mem_avg_temp_L1  1.953460e-02  0.080444  0.980996\n",
            "8        CNN            total_area  4.349302e-03  0.050438  0.995747\n",
            "9        CNN             avg_power  4.476855e-03  0.049863  0.995520\n",
            "10       CNN       avg_cores_power  4.272910e-03  0.049713  0.995754\n",
            "11       CNN     avg_routers_power  6.572783e-03  0.054973  0.993273\n",
            "12       CNN  avg_power_per_router  1.385876e-02  0.079468  0.985832\n",
            "13       CNN            layer_area  4.225792e-03  0.049325  0.995867\n",
            "14       CNN         area_per_core  7.736662e-09  0.000056  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "new_data = pd.DataFrame({\n",
        "    'dimx': [14],\n",
        "    'dimy': [11],\n",
        "    'dimz': [2],\n",
        "    'buffer_size': [10],\n",
        "    'packet_size_min': [4],\n",
        "    'packet_size_max': [8],\n",
        "    'routing_type_oe_3d': [1],  # Make sure to align with the encoded columns\n",
        "    'selection_strategy_thermal': [1],\n",
        "    'traffic_type_random': [1],\n",
        "    'injection_rate': [0.05]\n",
        "})\n",
        "\n",
        "# Ensure all columns are present (including any missing dummy variables from the training data)\n",
        "# Missing columns in new_data are filled with 0s\n",
        "for col in X.columns:\n",
        "    if col not in new_data:\n",
        "        new_data[col] = 0\n",
        "\n",
        "# Reorder the new_data columns to match the original feature order\n",
        "new_data = new_data[X.columns]\n",
        "\n",
        "# Scale the new data using the previously fitted scaler\n",
        "new_data_scaled = scaler_X.transform(new_data)\n",
        "\n",
        "# Reshape the scaled data to fit the CNN input shape\n",
        "new_data_cnn = new_data_scaled.reshape(1, new_data_scaled.shape[1], 1)  # Shape (1, num_features, 1)\n",
        "\n",
        "# Make predictions with the CNN model\n",
        "new_data_pred_scaled = cnn_model.predict(new_data_cnn)\n",
        "\n",
        "# Inverse scale the predictions to get original scale\n",
        "new_data_pred = scaler_y.inverse_transform(new_data_pred_scaled)\n",
        "\n",
        "# Convert the prediction to a DataFrame for readability\n",
        "predictions_df = pd.DataFrame(new_data_pred, columns=target_columns)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions for the new data:\")\n",
        "print(predictions_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RkYuAUPVyow",
        "outputId": "ba25dbef-bb7a-4c46-a164-f6a2aedb4f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "Predictions for the new data:\n",
            "   steady_state_temp_L0  steady_state_temp_L1  router_avg_temp_L0  \\\n",
            "0            141.590027            140.825592           26.857075   \n",
            "\n",
            "   router_avg_temp_L1  core_avg_temp_L0  core_avg_temp_L1  mem_avg_temp_L0  \\\n",
            "0           26.589748         26.159397         25.996588        25.862862   \n",
            "\n",
            "   mem_avg_temp_L1   total_area     avg_power  avg_cores_power  \\\n",
            "0        25.731989  729290496.0  4.319029e-07     3.490335e-07   \n",
            "\n",
            "   avg_routers_power  avg_power_per_router   layer_area  area_per_core  \n",
            "0       8.383054e-08          2.812783e-10  365363648.0      4695230.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Reshape the data to fit an RNN input (samples, time steps, features)\n",
        "X_train_rnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)  # 1 channel for each time step\n",
        "X_test_rnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)  # Same reshaping for test set\n",
        "\n",
        "# Build a deeper RNN model using multiple LSTM layers\n",
        "rnn_model = Sequential()\n",
        "\n",
        "# Add LSTM layers with more units and dropout for regularization\n",
        "rnn_model.add(LSTM(128, activation='relu', input_shape=(X_train_rnn.shape[1], 1), return_sequences=True))\n",
        "rnn_model.add(Dropout(0.2))  # Add dropout to prevent overfitting\n",
        "\n",
        "rnn_model.add(LSTM(256, activation='relu', return_sequences=True))  # Deeper layer\n",
        "rnn_model.add(Dropout(0.2))  # Add dropout to prevent overfitting\n",
        "\n",
        "rnn_model.add(LSTM(512, activation='relu', return_sequences=False))  # Even deeper LSTM layer\n",
        "rnn_model.add(Dropout(0.3))  # Higher dropout rate\n",
        "\n",
        "# Add dense layers for regression\n",
        "rnn_model.add(Dense(256, activation='relu'))  # Larger dense layer\n",
        "rnn_model.add(Dense(128, activation='relu'))  # Another dense layer\n",
        "rnn_model.add(Dense(64, activation='relu'))   # Another dense layer\n",
        "rnn_model.add(Dense(len(target_columns)))  # Output layer for all targets at once\n",
        "\n",
        "# Compile the RNN model\n",
        "rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "rnn_model.fit(X_train_rnn, y_train_scaled, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict using the trained RNN model\n",
        "y_pred_rnn_scaled = rnn_model.predict(X_test_rnn)\n",
        "\n",
        "# Prepare a list to store the evaluation results\n",
        "rnn_evaluation_results = []\n",
        "\n",
        "# Evaluate the model for each target variable separately\n",
        "print(\"Deeper RNN (LSTM) Performance (per target variable):\")\n",
        "for i, target in enumerate(target_columns):\n",
        "    y_test_target = y_test_scaled[:, i]  # Get actual values for target i\n",
        "    y_pred_target = y_pred_rnn_scaled[:, i]  # Get predicted values for target i\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(y_test_target, y_pred_target)\n",
        "    mae = mean_absolute_error(y_test_target, y_pred_target)\n",
        "    r2 = r2_score(y_test_target, y_pred_target)\n",
        "\n",
        "    # Append results to the list\n",
        "    rnn_evaluation_results.append({\n",
        "        'algorithm': 'Deeper RNN (LSTM)',\n",
        "        'target': target,\n",
        "        'MSE': mse,\n",
        "        'MAE': mae,\n",
        "        'R²': r2\n",
        "    })\n",
        "\n",
        "    # Print results for the current target\n",
        "    print(f\"\\nTarget: {target}\")\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R²:\", r2)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Convert the results list to a DataFrame\n",
        "rnn_evaluation_df = pd.DataFrame(rnn_evaluation_results)\n",
        "\n",
        "# Display the RNN evaluation results DataFrame\n",
        "print(\"\\nDeeper RNN Evaluation Results DataFrame:\")\n",
        "print(rnn_evaluation_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hCs38spd_-Z",
        "outputId": "d98dd75d-f2bf-40e2-fce6-4013ee52ba99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.6645 - val_loss: 0.5664\n",
            "Epoch 2/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - loss: 0.5028 - val_loss: 0.4097\n",
            "Epoch 3/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.3652 - val_loss: 0.3071\n",
            "Epoch 4/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2490 - val_loss: 0.1026\n",
            "Epoch 5/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.1034 - val_loss: 0.0504\n",
            "Epoch 6/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0703 - val_loss: 0.0983\n",
            "Epoch 7/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0864 - val_loss: 0.0688\n",
            "Epoch 8/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0663 - val_loss: 0.0313\n",
            "Epoch 9/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0487 - val_loss: 0.0272\n",
            "Epoch 10/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0400 - val_loss: 0.0256\n",
            "Epoch 11/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0424 - val_loss: 0.0534\n",
            "Epoch 12/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0551 - val_loss: 0.0180\n",
            "Epoch 13/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0316 - val_loss: 0.0185\n",
            "Epoch 14/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0309 - val_loss: 0.0922\n",
            "Epoch 15/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0415 - val_loss: 0.0141\n",
            "Epoch 16/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0341 - val_loss: 0.0190\n",
            "Epoch 17/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0328 - val_loss: 0.0161\n",
            "Epoch 18/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0238 - val_loss: 0.0134\n",
            "Epoch 19/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.0261 - val_loss: 0.0144\n",
            "Epoch 20/20\n",
            "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0243 - val_loss: 0.0145\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
            "Deeper RNN (LSTM) Performance (per target variable):\n",
            "\n",
            "Target: steady_state_temp_L0\n",
            "MSE: 0.018068839830149324\n",
            "MAE: 0.08126289865188807\n",
            "R²: 0.9811875897390472\n",
            "------------------------------\n",
            "\n",
            "Target: steady_state_temp_L1\n",
            "MSE: 0.01766832909145001\n",
            "MAE: 0.0800525616942383\n",
            "R²: 0.9816116497636261\n",
            "------------------------------\n",
            "\n",
            "Target: router_avg_temp_L0\n",
            "MSE: 0.022896850620749715\n",
            "MAE: 0.09402442586207033\n",
            "R²: 0.9770340214021885\n",
            "------------------------------\n",
            "\n",
            "Target: router_avg_temp_L1\n",
            "MSE: 0.015456613907882397\n",
            "MAE: 0.06532770039852967\n",
            "R²: 0.9852585481660856\n",
            "------------------------------\n",
            "\n",
            "Target: core_avg_temp_L0\n",
            "MSE: 0.039768176528913436\n",
            "MAE: 0.12693851049061408\n",
            "R²: 0.9596312812980912\n",
            "------------------------------\n",
            "\n",
            "Target: core_avg_temp_L1\n",
            "MSE: 0.022914964795090056\n",
            "MAE: 0.0896228632766653\n",
            "R²: 0.9773616199095354\n",
            "------------------------------\n",
            "\n",
            "Target: mem_avg_temp_L0\n",
            "MSE: 0.03229964109864727\n",
            "MAE: 0.11700791733353215\n",
            "R²: 0.9678542666138445\n",
            "------------------------------\n",
            "\n",
            "Target: mem_avg_temp_L1\n",
            "MSE: 0.01662288975958161\n",
            "MAE: 0.07741612451858772\n",
            "R²: 0.9838282891583429\n",
            "------------------------------\n",
            "\n",
            "Target: total_area\n",
            "MSE: 0.0023399125792219417\n",
            "MAE: 0.03541235326418089\n",
            "R²: 0.9977117368741231\n",
            "------------------------------\n",
            "\n",
            "Target: avg_power\n",
            "MSE: 0.0038799769676717876\n",
            "MAE: 0.042352006882259374\n",
            "R²: 0.9961169093915152\n",
            "------------------------------\n",
            "\n",
            "Target: avg_cores_power\n",
            "MSE: 0.0029711432264539596\n",
            "MAE: 0.03900372704552554\n",
            "R²: 0.9970475166800402\n",
            "------------------------------\n",
            "\n",
            "Target: avg_routers_power\n",
            "MSE: 0.007508394559296267\n",
            "MAE: 0.05040857127731262\n",
            "R²: 0.9923149367747259\n",
            "------------------------------\n",
            "\n",
            "Target: avg_power_per_router\n",
            "MSE: 0.019652956255071487\n",
            "MAE: 0.09470701571058943\n",
            "R²: 0.979908718587354\n",
            "------------------------------\n",
            "\n",
            "Target: layer_area\n",
            "MSE: 0.0024702542610824\n",
            "MAE: 0.037497478939668795\n",
            "R²: 0.9975842716722385\n",
            "------------------------------\n",
            "\n",
            "Target: area_per_core\n",
            "MSE: 1.9757073705458773e-05\n",
            "MAE: 0.0036110957539838904\n",
            "R²: 0.0\n",
            "------------------------------\n",
            "\n",
            "Deeper RNN Evaluation Results DataFrame:\n",
            "            algorithm                target       MSE       MAE        R²\n",
            "0   Deeper RNN (LSTM)  steady_state_temp_L0  0.018069  0.081263  0.981188\n",
            "1   Deeper RNN (LSTM)  steady_state_temp_L1  0.017668  0.080053  0.981612\n",
            "2   Deeper RNN (LSTM)    router_avg_temp_L0  0.022897  0.094024  0.977034\n",
            "3   Deeper RNN (LSTM)    router_avg_temp_L1  0.015457  0.065328  0.985259\n",
            "4   Deeper RNN (LSTM)      core_avg_temp_L0  0.039768  0.126939  0.959631\n",
            "5   Deeper RNN (LSTM)      core_avg_temp_L1  0.022915  0.089623  0.977362\n",
            "6   Deeper RNN (LSTM)       mem_avg_temp_L0  0.032300  0.117008  0.967854\n",
            "7   Deeper RNN (LSTM)       mem_avg_temp_L1  0.016623  0.077416  0.983828\n",
            "8   Deeper RNN (LSTM)            total_area  0.002340  0.035412  0.997712\n",
            "9   Deeper RNN (LSTM)             avg_power  0.003880  0.042352  0.996117\n",
            "10  Deeper RNN (LSTM)       avg_cores_power  0.002971  0.039004  0.997048\n",
            "11  Deeper RNN (LSTM)     avg_routers_power  0.007508  0.050409  0.992315\n",
            "12  Deeper RNN (LSTM)  avg_power_per_router  0.019653  0.094707  0.979909\n",
            "13  Deeper RNN (LSTM)            layer_area  0.002470  0.037497  0.997584\n",
            "14  Deeper RNN (LSTM)         area_per_core  0.000020  0.003611  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "new_data = pd.DataFrame({\n",
        "    'dimx': [3],\n",
        "    'dimy': [10],\n",
        "    'dimz': [2],\n",
        "    'buffer_size': [8],\n",
        "    'packet_size_min': [4],\n",
        "    'packet_size_max': [8],\n",
        "    'routing_type_fullyadaptive': [1],    # Make sure to align with the encoded columns\n",
        "    'selection_strategy_thermal': [1],\n",
        "    'traffic_type_random': [1],\n",
        "    'injection_rate': [0.06]\n",
        "})\n",
        "\n",
        "# Ensure all columns are present (including any missing dummy variables from the training data)\n",
        "# Missing columns in new_data are filled with 0s\n",
        "for col in X.columns:\n",
        "    if col not in new_data:\n",
        "        new_data[col] = 0\n",
        "\n",
        "# Reorder the new_data columns to match the original feature order\n",
        "new_data = new_data[X.columns]\n",
        "\n",
        "# Scale the new data using the previously fitted scaler\n",
        "new_data_scaled = scaler_X.transform(new_data)\n",
        "\n",
        "# Reshape the scaled data to fit the CNN input shape\n",
        "new_data_cnn = new_data_scaled.reshape(1, new_data_scaled.shape[1], 1)  # Shape (1, num_features, 1)\n",
        "\n",
        "# Make predictions with the CNN model\n",
        "new_data_pred_scaled = rnn_model.predict(new_data_cnn)\n",
        "\n",
        "# Inverse scale the predictions to get original scale\n",
        "new_data_pred = scaler_y.inverse_transform(new_data_pred_scaled)\n",
        "\n",
        "# Convert the prediction to a DataFrame for readability\n",
        "predictions_df = pd.DataFrame(new_data_pred, columns=target_columns)\n",
        "\n",
        "# Display the predictions\n",
        "print(\"Predictions for the new data:\")\n",
        "print(predictions_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQRK2scRe3Go",
        "outputId": "15745ecf-e0fa-41cb-cf9b-c6d0b39db19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Predictions for the new data:\n",
            "   steady_state_temp_L0  steady_state_temp_L1  router_avg_temp_L0  \\\n",
            "0             67.197083             67.573349           25.606779   \n",
            "\n",
            "   router_avg_temp_L1  core_avg_temp_L0  core_avg_temp_L1  mem_avg_temp_L0  \\\n",
            "0           25.979721         25.475954         25.574236         25.37184   \n",
            "\n",
            "   mem_avg_temp_L1   total_area     avg_power  avg_cores_power  \\\n",
            "0        25.447403  160086704.0  8.112868e-08     7.021715e-08   \n",
            "\n",
            "   avg_routers_power  avg_power_per_router  layer_area  area_per_core  \n",
            "0       1.132547e-08          1.961568e-10  75313344.0      4695230.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('PAT_Dataset.csv')\n",
        "\n",
        "# Columns containing the target variables\n",
        "temperature_columns = [\n",
        "    'steady_state_temp_L0', 'steady_state_temp_L1', 'router_avg_temp_L0', 'router_avg_temp_L1',\n",
        "    'core_avg_temp_L0', 'core_avg_temp_L1', 'mem_avg_temp_L0', 'mem_avg_temp_L1'\n",
        "]\n",
        "\n",
        "non_temperature_columns = [\n",
        "    'total_area', 'avg_power', 'avg_cores_power', 'avg_routers_power', 'avg_power_per_router',\n",
        "    'layer_area', 'area_per_core'\n",
        "]\n",
        "\n",
        "# One-hot encode categorical columns\n",
        "categorical_columns = ['routing_type', 'selection_strategy', 'traffic_type']\n",
        "X = pd.get_dummies(df.drop(columns=temperature_columns + non_temperature_columns), columns=categorical_columns, drop_first=True)\n",
        "y_temp = df[temperature_columns]\n",
        "y_non_temp = df[non_temperature_columns]\n",
        "\n",
        "# Scale features and targets\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y_temp = StandardScaler()\n",
        "scaler_y_non_temp = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_temp_scaled = scaler_y_temp.fit_transform(y_temp)\n",
        "y_non_temp_scaled = scaler_y_non_temp.fit_transform(y_non_temp)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_temp_train, y_temp_test, y_non_temp_train, y_non_temp_test = train_test_split(\n",
        "    X_scaled, y_temp_scaled, y_non_temp_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the FNN Model for Temperature Predictions\n",
        "def create_fnn_model(input_dim, output_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation='relu', input_dim=input_dim),\n",
        "        tf.keras.layers.BatchNormalization(),  # Batch Normalization\n",
        "        tf.keras.layers.Dropout(0.3),  # Dropout Regularization\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Dense(output_dim)  # Regression output layer\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Instantiate and train the FNN model\n",
        "fnn_model = create_fnn_model(X_train.shape[1], y_temp_train.shape[1])\n",
        "fnn_model.fit(X_train, y_temp_train, epochs=200, batch_size=32, validation_data=(X_test, y_temp_test))\n",
        "\n",
        "# Predict temperature values\n",
        "y_temp_pred_scaled = fnn_model.predict(X_test)\n",
        "\n",
        "# Define and train the Random Forest Model for Non-Temperature Predictions\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_non_temp_train)\n",
        "\n",
        "# Predict non-temperature values\n",
        "y_non_temp_pred_scaled = rf_model.predict(X_test)\n",
        "\n",
        "# Combine the predictions (Temperature + Non-Temperature)\n",
        "y_combined_pred_scaled = np.hstack((y_temp_pred_scaled, y_non_temp_pred_scaled))\n",
        "\n",
        "# Calculate and print MSE, MAE, and R² for each column directly on scaled values\n",
        "print(\"Metrics on Scaled Data (No inverse transform):\")\n",
        "# Temperature-related metrics\n",
        "for i, col in enumerate(temperature_columns):\n",
        "    mse = mean_squared_error(y_temp_test[:, i], y_temp_pred_scaled[:, i])\n",
        "    mae = mean_absolute_error(y_temp_test[:, i], y_temp_pred_scaled[:, i])\n",
        "    r2 = r2_score(y_temp_test[:, i], y_temp_pred_scaled[:, i])\n",
        "    print(f\"Temperature - {col}:\")\n",
        "    print(f\"  MSE: {mse:.4f}\")\n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "    print(f\"  R²: {r2:.4f}\")\n",
        "\n",
        "# Non-temperature-related metrics\n",
        "for i, col in enumerate(non_temperature_columns):\n",
        "    mse = mean_squared_error(y_non_temp_test[:, i], y_non_temp_pred_scaled[:, i])\n",
        "    mae = mean_absolute_error(y_non_temp_test[:, i], y_non_temp_pred_scaled[:, i])\n",
        "    r2 = r2_score(y_non_temp_test[:, i], y_non_temp_pred_scaled[:, i])\n",
        "    print(f\"Non-Temperature - {col}:\")\n",
        "    print(f\"  MSE: {mse:.4f}\")\n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "    print(f\"  R²: {r2:.4f}\")\n",
        "\n",
        "# Combined Metrics (overall performance on all targets)\n",
        "combined_mse = mean_squared_error(np.hstack((y_temp_test, y_non_temp_test)), y_combined_pred_scaled)\n",
        "combined_mae = mean_absolute_error(np.hstack((y_temp_test, y_non_temp_test)), y_combined_pred_scaled)\n",
        "combined_r2 = r2_score(np.hstack((y_temp_test, y_non_temp_test)), y_combined_pred_scaled)\n",
        "\n",
        "print(\"\\nCombined Metrics (Temperature + Non-Temperature):\")\n",
        "print(f\"  MSE: {combined_mse:.4f}\")\n",
        "print(f\"  MAE: {combined_mae:.4f}\")\n",
        "print(f\"  R²: {combined_r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kWcys7u74oj",
        "outputId": "fef68942-4b83-4829-9016-8e6869a01ff7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 1.5724 - mae: 0.8966 - val_loss: 0.3993 - val_mae: 0.3425\n",
            "Epoch 2/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.5526 - mae: 0.4724 - val_loss: 0.2410 - val_mae: 0.2608\n",
            "Epoch 3/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3902 - mae: 0.3927 - val_loss: 0.1922 - val_mae: 0.2300\n",
            "Epoch 4/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3409 - mae: 0.3696 - val_loss: 0.2071 - val_mae: 0.2257\n",
            "Epoch 5/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3117 - mae: 0.3577 - val_loss: 0.1639 - val_mae: 0.2080\n",
            "Epoch 6/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3019 - mae: 0.3508 - val_loss: 0.1467 - val_mae: 0.1979\n",
            "Epoch 7/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2839 - mae: 0.3420 - val_loss: 0.1403 - val_mae: 0.1851\n",
            "Epoch 8/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2667 - mae: 0.3357 - val_loss: 0.1286 - val_mae: 0.1861\n",
            "Epoch 9/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2526 - mae: 0.3269 - val_loss: 0.1279 - val_mae: 0.1858\n",
            "Epoch 10/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2516 - mae: 0.3270 - val_loss: 0.1090 - val_mae: 0.1688\n",
            "Epoch 11/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.2461 - mae: 0.3243 - val_loss: 0.1112 - val_mae: 0.1736\n",
            "Epoch 12/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2441 - mae: 0.3255 - val_loss: 0.0939 - val_mae: 0.1589\n",
            "Epoch 13/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2338 - mae: 0.3187 - val_loss: 0.1230 - val_mae: 0.1685\n",
            "Epoch 14/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2267 - mae: 0.3162 - val_loss: 0.1073 - val_mae: 0.1730\n",
            "Epoch 15/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2205 - mae: 0.3111 - val_loss: 0.0809 - val_mae: 0.1542\n",
            "Epoch 16/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2379 - mae: 0.3192 - val_loss: 0.0811 - val_mae: 0.1579\n",
            "Epoch 17/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.2254 - mae: 0.3147 - val_loss: 0.0834 - val_mae: 0.1625\n",
            "Epoch 18/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2226 - mae: 0.3164 - val_loss: 0.0708 - val_mae: 0.1549\n",
            "Epoch 19/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2094 - mae: 0.3086 - val_loss: 0.0817 - val_mae: 0.1669\n",
            "Epoch 20/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2154 - mae: 0.3099 - val_loss: 0.0615 - val_mae: 0.1467\n",
            "Epoch 21/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2087 - mae: 0.3065 - val_loss: 0.0530 - val_mae: 0.1364\n",
            "Epoch 22/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2084 - mae: 0.3056 - val_loss: 0.0638 - val_mae: 0.1498\n",
            "Epoch 23/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.2004 - mae: 0.3045 - val_loss: 0.0518 - val_mae: 0.1372\n",
            "Epoch 24/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.2013 - mae: 0.3034 - val_loss: 0.0610 - val_mae: 0.1498\n",
            "Epoch 25/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1998 - mae: 0.3046 - val_loss: 0.0507 - val_mae: 0.1385\n",
            "Epoch 26/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1925 - mae: 0.2967 - val_loss: 0.0526 - val_mae: 0.1450\n",
            "Epoch 27/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.2018 - mae: 0.3034 - val_loss: 0.0476 - val_mae: 0.1354\n",
            "Epoch 28/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1923 - mae: 0.2984 - val_loss: 0.0545 - val_mae: 0.1481\n",
            "Epoch 29/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1905 - mae: 0.3023 - val_loss: 0.0579 - val_mae: 0.1426\n",
            "Epoch 30/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1972 - mae: 0.3001 - val_loss: 0.0676 - val_mae: 0.1485\n",
            "Epoch 31/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1988 - mae: 0.3022 - val_loss: 0.0506 - val_mae: 0.1429\n",
            "Epoch 32/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1960 - mae: 0.3025 - val_loss: 0.0480 - val_mae: 0.1385\n",
            "Epoch 33/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1868 - mae: 0.2956 - val_loss: 0.0494 - val_mae: 0.1402\n",
            "Epoch 34/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1792 - mae: 0.2923 - val_loss: 0.0502 - val_mae: 0.1376\n",
            "Epoch 35/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1763 - mae: 0.2917 - val_loss: 0.0520 - val_mae: 0.1401\n",
            "Epoch 36/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1896 - mae: 0.2982 - val_loss: 0.1044 - val_mae: 0.1628\n",
            "Epoch 37/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1822 - mae: 0.2939 - val_loss: 0.0486 - val_mae: 0.1380\n",
            "Epoch 38/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1930 - mae: 0.2991 - val_loss: 0.0520 - val_mae: 0.1462\n",
            "Epoch 39/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1874 - mae: 0.2991 - val_loss: 0.0411 - val_mae: 0.1290\n",
            "Epoch 40/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1850 - mae: 0.2976 - val_loss: 0.0469 - val_mae: 0.1362\n",
            "Epoch 41/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1795 - mae: 0.2905 - val_loss: 0.0500 - val_mae: 0.1335\n",
            "Epoch 42/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1798 - mae: 0.2930 - val_loss: 0.0491 - val_mae: 0.1396\n",
            "Epoch 43/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1801 - mae: 0.2944 - val_loss: 0.0521 - val_mae: 0.1446\n",
            "Epoch 44/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1779 - mae: 0.2943 - val_loss: 0.0479 - val_mae: 0.1319\n",
            "Epoch 45/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1903 - mae: 0.2993 - val_loss: 0.0534 - val_mae: 0.1381\n",
            "Epoch 46/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1807 - mae: 0.2906 - val_loss: 0.0461 - val_mae: 0.1430\n",
            "Epoch 47/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1786 - mae: 0.2919 - val_loss: 0.0430 - val_mae: 0.1312\n",
            "Epoch 48/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1828 - mae: 0.2948 - val_loss: 0.0486 - val_mae: 0.1423\n",
            "Epoch 49/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1694 - mae: 0.2894 - val_loss: 0.0435 - val_mae: 0.1331\n",
            "Epoch 50/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1763 - mae: 0.2916 - val_loss: 0.0426 - val_mae: 0.1317\n",
            "Epoch 51/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1756 - mae: 0.2925 - val_loss: 0.0477 - val_mae: 0.1351\n",
            "Epoch 52/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1779 - mae: 0.2923 - val_loss: 0.0405 - val_mae: 0.1326\n",
            "Epoch 53/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1752 - mae: 0.2897 - val_loss: 0.0388 - val_mae: 0.1249\n",
            "Epoch 54/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1835 - mae: 0.2985 - val_loss: 0.0394 - val_mae: 0.1288\n",
            "Epoch 55/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1749 - mae: 0.2920 - val_loss: 0.0494 - val_mae: 0.1418\n",
            "Epoch 56/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1755 - mae: 0.2908 - val_loss: 0.0423 - val_mae: 0.1298\n",
            "Epoch 57/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1810 - mae: 0.2941 - val_loss: 0.0526 - val_mae: 0.1411\n",
            "Epoch 58/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1674 - mae: 0.2880 - val_loss: 0.0561 - val_mae: 0.1479\n",
            "Epoch 59/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1711 - mae: 0.2910 - val_loss: 0.0369 - val_mae: 0.1221\n",
            "Epoch 60/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1702 - mae: 0.2868 - val_loss: 0.0426 - val_mae: 0.1407\n",
            "Epoch 61/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1782 - mae: 0.2949 - val_loss: 0.0470 - val_mae: 0.1373\n",
            "Epoch 62/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1862 - mae: 0.2956 - val_loss: 0.0453 - val_mae: 0.1352\n",
            "Epoch 63/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1724 - mae: 0.2895 - val_loss: 0.0366 - val_mae: 0.1236\n",
            "Epoch 64/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1701 - mae: 0.2871 - val_loss: 0.0415 - val_mae: 0.1330\n",
            "Epoch 65/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1706 - mae: 0.2879 - val_loss: 0.0385 - val_mae: 0.1280\n",
            "Epoch 66/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1748 - mae: 0.2912 - val_loss: 0.0466 - val_mae: 0.1424\n",
            "Epoch 67/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1698 - mae: 0.2870 - val_loss: 0.0377 - val_mae: 0.1238\n",
            "Epoch 68/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1733 - mae: 0.2926 - val_loss: 0.0538 - val_mae: 0.1404\n",
            "Epoch 69/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1656 - mae: 0.2872 - val_loss: 0.0603 - val_mae: 0.1391\n",
            "Epoch 70/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1734 - mae: 0.2893 - val_loss: 0.0493 - val_mae: 0.1353\n",
            "Epoch 71/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1766 - mae: 0.2925 - val_loss: 0.0580 - val_mae: 0.1431\n",
            "Epoch 72/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1623 - mae: 0.2807 - val_loss: 0.0509 - val_mae: 0.1420\n",
            "Epoch 73/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1725 - mae: 0.2912 - val_loss: 0.0512 - val_mae: 0.1373\n",
            "Epoch 74/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1700 - mae: 0.2840 - val_loss: 0.0345 - val_mae: 0.1216\n",
            "Epoch 75/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1653 - mae: 0.2847 - val_loss: 0.0476 - val_mae: 0.1430\n",
            "Epoch 76/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1780 - mae: 0.2920 - val_loss: 0.0374 - val_mae: 0.1239\n",
            "Epoch 77/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1684 - mae: 0.2877 - val_loss: 0.0467 - val_mae: 0.1300\n",
            "Epoch 78/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1692 - mae: 0.2863 - val_loss: 0.0408 - val_mae: 0.1314\n",
            "Epoch 79/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1695 - mae: 0.2886 - val_loss: 0.0389 - val_mae: 0.1288\n",
            "Epoch 80/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1711 - mae: 0.2864 - val_loss: 0.0516 - val_mae: 0.1336\n",
            "Epoch 81/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1779 - mae: 0.2917 - val_loss: 0.0361 - val_mae: 0.1240\n",
            "Epoch 82/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1746 - mae: 0.2889 - val_loss: 0.0385 - val_mae: 0.1236\n",
            "Epoch 83/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1643 - mae: 0.2854 - val_loss: 0.0362 - val_mae: 0.1258\n",
            "Epoch 84/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1747 - mae: 0.2896 - val_loss: 0.0407 - val_mae: 0.1323\n",
            "Epoch 85/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1660 - mae: 0.2841 - val_loss: 0.0881 - val_mae: 0.1471\n",
            "Epoch 86/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1704 - mae: 0.2860 - val_loss: 0.0373 - val_mae: 0.1226\n",
            "Epoch 87/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1654 - mae: 0.2867 - val_loss: 0.0424 - val_mae: 0.1310\n",
            "Epoch 88/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1642 - mae: 0.2843 - val_loss: 0.0434 - val_mae: 0.1384\n",
            "Epoch 89/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1691 - mae: 0.2865 - val_loss: 0.0533 - val_mae: 0.1449\n",
            "Epoch 90/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1613 - mae: 0.2846 - val_loss: 0.0670 - val_mae: 0.1411\n",
            "Epoch 91/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1743 - mae: 0.2885 - val_loss: 0.0414 - val_mae: 0.1309\n",
            "Epoch 92/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1718 - mae: 0.2883 - val_loss: 0.0482 - val_mae: 0.1394\n",
            "Epoch 93/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1681 - mae: 0.2842 - val_loss: 0.0407 - val_mae: 0.1292\n",
            "Epoch 94/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1701 - mae: 0.2870 - val_loss: 0.0395 - val_mae: 0.1281\n",
            "Epoch 95/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1657 - mae: 0.2859 - val_loss: 0.0321 - val_mae: 0.1163\n",
            "Epoch 96/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1655 - mae: 0.2858 - val_loss: 0.0414 - val_mae: 0.1292\n",
            "Epoch 97/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1605 - mae: 0.2830 - val_loss: 0.0341 - val_mae: 0.1220\n",
            "Epoch 98/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1656 - mae: 0.2832 - val_loss: 0.0476 - val_mae: 0.1324\n",
            "Epoch 99/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.1655 - mae: 0.2856 - val_loss: 0.0512 - val_mae: 0.1308\n",
            "Epoch 100/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1639 - mae: 0.2828 - val_loss: 0.0383 - val_mae: 0.1292\n",
            "Epoch 101/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1681 - mae: 0.2862 - val_loss: 0.0435 - val_mae: 0.1333\n",
            "Epoch 102/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1611 - mae: 0.2799 - val_loss: 0.0346 - val_mae: 0.1238\n",
            "Epoch 103/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1647 - mae: 0.2855 - val_loss: 0.0438 - val_mae: 0.1333\n",
            "Epoch 104/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1694 - mae: 0.2872 - val_loss: 0.0350 - val_mae: 0.1213\n",
            "Epoch 105/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1614 - mae: 0.2836 - val_loss: 0.0483 - val_mae: 0.1336\n",
            "Epoch 106/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1608 - mae: 0.2829 - val_loss: 0.0394 - val_mae: 0.1271\n",
            "Epoch 107/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1631 - mae: 0.2833 - val_loss: 0.0342 - val_mae: 0.1178\n",
            "Epoch 108/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.1633 - mae: 0.2859 - val_loss: 0.0441 - val_mae: 0.1316\n",
            "Epoch 109/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1674 - mae: 0.2864 - val_loss: 0.0423 - val_mae: 0.1303\n",
            "Epoch 110/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1696 - mae: 0.2889 - val_loss: 0.0412 - val_mae: 0.1350\n",
            "Epoch 111/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1646 - mae: 0.2820 - val_loss: 0.0356 - val_mae: 0.1229\n",
            "Epoch 112/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1642 - mae: 0.2859 - val_loss: 0.0365 - val_mae: 0.1226\n",
            "Epoch 113/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1624 - mae: 0.2836 - val_loss: 0.0389 - val_mae: 0.1310\n",
            "Epoch 114/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1713 - mae: 0.2909 - val_loss: 0.0467 - val_mae: 0.1315\n",
            "Epoch 115/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1711 - mae: 0.2901 - val_loss: 0.0522 - val_mae: 0.1352\n",
            "Epoch 116/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1676 - mae: 0.2856 - val_loss: 0.0602 - val_mae: 0.1513\n",
            "Epoch 117/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1661 - mae: 0.2846 - val_loss: 0.0453 - val_mae: 0.1297\n",
            "Epoch 118/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1677 - mae: 0.2865 - val_loss: 0.0353 - val_mae: 0.1281\n",
            "Epoch 119/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1622 - mae: 0.2862 - val_loss: 0.0423 - val_mae: 0.1278\n",
            "Epoch 120/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1615 - mae: 0.2823 - val_loss: 0.0365 - val_mae: 0.1233\n",
            "Epoch 121/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1665 - mae: 0.2864 - val_loss: 0.0380 - val_mae: 0.1235\n",
            "Epoch 122/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1589 - mae: 0.2837 - val_loss: 0.0453 - val_mae: 0.1231\n",
            "Epoch 123/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1655 - mae: 0.2856 - val_loss: 0.0360 - val_mae: 0.1244\n",
            "Epoch 124/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1591 - mae: 0.2798 - val_loss: 0.0393 - val_mae: 0.1271\n",
            "Epoch 125/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1623 - mae: 0.2831 - val_loss: 0.0462 - val_mae: 0.1295\n",
            "Epoch 126/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1601 - mae: 0.2817 - val_loss: 0.0364 - val_mae: 0.1242\n",
            "Epoch 127/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1612 - mae: 0.2834 - val_loss: 0.0475 - val_mae: 0.1463\n",
            "Epoch 128/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1622 - mae: 0.2855 - val_loss: 0.0360 - val_mae: 0.1244\n",
            "Epoch 129/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1597 - mae: 0.2840 - val_loss: 0.0442 - val_mae: 0.1275\n",
            "Epoch 130/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1596 - mae: 0.2790 - val_loss: 0.0531 - val_mae: 0.1404\n",
            "Epoch 131/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1681 - mae: 0.2826 - val_loss: 0.0358 - val_mae: 0.1215\n",
            "Epoch 132/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1578 - mae: 0.2798 - val_loss: 0.0361 - val_mae: 0.1282\n",
            "Epoch 133/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1670 - mae: 0.2858 - val_loss: 0.0443 - val_mae: 0.1257\n",
            "Epoch 134/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1680 - mae: 0.2883 - val_loss: 0.0352 - val_mae: 0.1210\n",
            "Epoch 135/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1618 - mae: 0.2852 - val_loss: 0.0574 - val_mae: 0.1392\n",
            "Epoch 136/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1589 - mae: 0.2808 - val_loss: 0.0375 - val_mae: 0.1233\n",
            "Epoch 137/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1577 - mae: 0.2819 - val_loss: 0.0322 - val_mae: 0.1216\n",
            "Epoch 138/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1566 - mae: 0.2791 - val_loss: 0.0432 - val_mae: 0.1334\n",
            "Epoch 139/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1668 - mae: 0.2873 - val_loss: 0.0443 - val_mae: 0.1309\n",
            "Epoch 140/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1537 - mae: 0.2767 - val_loss: 0.0421 - val_mae: 0.1293\n",
            "Epoch 141/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1622 - mae: 0.2804 - val_loss: 0.0485 - val_mae: 0.1285\n",
            "Epoch 142/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1615 - mae: 0.2816 - val_loss: 0.0406 - val_mae: 0.1253\n",
            "Epoch 143/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1505 - mae: 0.2739 - val_loss: 0.0396 - val_mae: 0.1323\n",
            "Epoch 144/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1666 - mae: 0.2829 - val_loss: 0.0445 - val_mae: 0.1386\n",
            "Epoch 145/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1570 - mae: 0.2810 - val_loss: 0.0374 - val_mae: 0.1270\n",
            "Epoch 146/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1588 - mae: 0.2805 - val_loss: 0.0377 - val_mae: 0.1267\n",
            "Epoch 147/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1586 - mae: 0.2821 - val_loss: 0.0415 - val_mae: 0.1313\n",
            "Epoch 148/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1620 - mae: 0.2839 - val_loss: 0.0396 - val_mae: 0.1266\n",
            "Epoch 149/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1571 - mae: 0.2789 - val_loss: 0.0333 - val_mae: 0.1225\n",
            "Epoch 150/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1578 - mae: 0.2794 - val_loss: 0.0401 - val_mae: 0.1327\n",
            "Epoch 151/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1667 - mae: 0.2866 - val_loss: 0.0374 - val_mae: 0.1250\n",
            "Epoch 152/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1611 - mae: 0.2800 - val_loss: 0.0479 - val_mae: 0.1414\n",
            "Epoch 153/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1622 - mae: 0.2830 - val_loss: 0.0291 - val_mae: 0.1131\n",
            "Epoch 154/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1662 - mae: 0.2847 - val_loss: 0.0372 - val_mae: 0.1265\n",
            "Epoch 155/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1598 - mae: 0.2825 - val_loss: 0.0410 - val_mae: 0.1278\n",
            "Epoch 156/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1603 - mae: 0.2849 - val_loss: 0.0317 - val_mae: 0.1198\n",
            "Epoch 157/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1638 - mae: 0.2857 - val_loss: 0.0412 - val_mae: 0.1274\n",
            "Epoch 158/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1513 - mae: 0.2753 - val_loss: 0.0332 - val_mae: 0.1177\n",
            "Epoch 159/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1606 - mae: 0.2826 - val_loss: 0.0395 - val_mae: 0.1294\n",
            "Epoch 160/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1698 - mae: 0.2897 - val_loss: 0.0515 - val_mae: 0.1385\n",
            "Epoch 161/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1585 - mae: 0.2778 - val_loss: 0.0375 - val_mae: 0.1265\n",
            "Epoch 162/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1553 - mae: 0.2799 - val_loss: 0.0369 - val_mae: 0.1174\n",
            "Epoch 163/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1543 - mae: 0.2768 - val_loss: 0.0372 - val_mae: 0.1275\n",
            "Epoch 164/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1632 - mae: 0.2850 - val_loss: 0.0596 - val_mae: 0.1363\n",
            "Epoch 165/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1565 - mae: 0.2789 - val_loss: 0.0437 - val_mae: 0.1278\n",
            "Epoch 166/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1584 - mae: 0.2788 - val_loss: 0.0453 - val_mae: 0.1363\n",
            "Epoch 167/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1654 - mae: 0.2858 - val_loss: 0.0336 - val_mae: 0.1203\n",
            "Epoch 168/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1569 - mae: 0.2788 - val_loss: 0.0340 - val_mae: 0.1207\n",
            "Epoch 169/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1471 - mae: 0.2745 - val_loss: 0.0378 - val_mae: 0.1263\n",
            "Epoch 170/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1561 - mae: 0.2808 - val_loss: 0.0347 - val_mae: 0.1216\n",
            "Epoch 171/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1563 - mae: 0.2815 - val_loss: 0.0469 - val_mae: 0.1368\n",
            "Epoch 172/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1547 - mae: 0.2761 - val_loss: 0.0358 - val_mae: 0.1164\n",
            "Epoch 173/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1556 - mae: 0.2771 - val_loss: 0.0476 - val_mae: 0.1340\n",
            "Epoch 174/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1620 - mae: 0.2821 - val_loss: 0.0367 - val_mae: 0.1165\n",
            "Epoch 175/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1514 - mae: 0.2770 - val_loss: 0.0438 - val_mae: 0.1308\n",
            "Epoch 176/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1517 - mae: 0.2763 - val_loss: 0.0570 - val_mae: 0.1412\n",
            "Epoch 177/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1584 - mae: 0.2799 - val_loss: 0.0327 - val_mae: 0.1216\n",
            "Epoch 178/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1533 - mae: 0.2778 - val_loss: 0.0399 - val_mae: 0.1291\n",
            "Epoch 179/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1487 - mae: 0.2749 - val_loss: 0.0379 - val_mae: 0.1231\n",
            "Epoch 180/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1519 - mae: 0.2770 - val_loss: 0.0356 - val_mae: 0.1243\n",
            "Epoch 181/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1624 - mae: 0.2850 - val_loss: 0.0384 - val_mae: 0.1218\n",
            "Epoch 182/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1639 - mae: 0.2828 - val_loss: 0.0375 - val_mae: 0.1314\n",
            "Epoch 183/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1583 - mae: 0.2826 - val_loss: 0.0360 - val_mae: 0.1258\n",
            "Epoch 184/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1581 - mae: 0.2812 - val_loss: 0.0404 - val_mae: 0.1280\n",
            "Epoch 185/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1572 - mae: 0.2820 - val_loss: 0.0395 - val_mae: 0.1238\n",
            "Epoch 186/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1623 - mae: 0.2841 - val_loss: 0.0378 - val_mae: 0.1237\n",
            "Epoch 187/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1536 - mae: 0.2759 - val_loss: 0.0357 - val_mae: 0.1212\n",
            "Epoch 188/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1462 - mae: 0.2728 - val_loss: 0.0409 - val_mae: 0.1317\n",
            "Epoch 189/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1531 - mae: 0.2775 - val_loss: 0.0386 - val_mae: 0.1295\n",
            "Epoch 190/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.1582 - mae: 0.2827 - val_loss: 0.0358 - val_mae: 0.1236\n",
            "Epoch 191/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1501 - mae: 0.2748 - val_loss: 0.0412 - val_mae: 0.1368\n",
            "Epoch 192/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1532 - mae: 0.2772 - val_loss: 0.0385 - val_mae: 0.1234\n",
            "Epoch 193/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1543 - mae: 0.2800 - val_loss: 0.0311 - val_mae: 0.1175\n",
            "Epoch 194/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1527 - mae: 0.2779 - val_loss: 0.0314 - val_mae: 0.1147\n",
            "Epoch 195/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1444 - mae: 0.2699 - val_loss: 0.0360 - val_mae: 0.1229\n",
            "Epoch 196/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1685 - mae: 0.2872 - val_loss: 0.0314 - val_mae: 0.1198\n",
            "Epoch 197/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1528 - mae: 0.2778 - val_loss: 0.0368 - val_mae: 0.1250\n",
            "Epoch 198/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1495 - mae: 0.2751 - val_loss: 0.0359 - val_mae: 0.1220\n",
            "Epoch 199/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.1530 - mae: 0.2765 - val_loss: 0.0348 - val_mae: 0.1197\n",
            "Epoch 200/200\n",
            "\u001b[1m645/645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.1560 - mae: 0.2804 - val_loss: 0.0379 - val_mae: 0.1250\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "Metrics on Scaled Data (No inverse transform):\n",
            "Temperature - steady_state_temp_L0:\n",
            "  MSE: 0.0360\n",
            "  MAE: 0.1298\n",
            "  R²: 0.9635\n",
            "Temperature - steady_state_temp_L1:\n",
            "  MSE: 0.0362\n",
            "  MAE: 0.1301\n",
            "  R²: 0.9633\n",
            "Temperature - router_avg_temp_L0:\n",
            "  MSE: 0.0416\n",
            "  MAE: 0.1258\n",
            "  R²: 0.9593\n",
            "Temperature - router_avg_temp_L1:\n",
            "  MSE: 0.0379\n",
            "  MAE: 0.0851\n",
            "  R²: 0.9630\n",
            "Temperature - core_avg_temp_L0:\n",
            "  MSE: 0.0378\n",
            "  MAE: 0.1323\n",
            "  R²: 0.9627\n",
            "Temperature - core_avg_temp_L1:\n",
            "  MSE: 0.0310\n",
            "  MAE: 0.1200\n",
            "  R²: 0.9696\n",
            "Temperature - mem_avg_temp_L0:\n",
            "  MSE: 0.0510\n",
            "  MAE: 0.1589\n",
            "  R²: 0.9511\n",
            "Temperature - mem_avg_temp_L1:\n",
            "  MSE: 0.0314\n",
            "  MAE: 0.1182\n",
            "  R²: 0.9695\n",
            "Non-Temperature - total_area:\n",
            "  MSE: 0.0001\n",
            "  MAE: 0.0036\n",
            "  R²: 0.9999\n",
            "Non-Temperature - avg_power:\n",
            "  MSE: 0.0006\n",
            "  MAE: 0.0096\n",
            "  R²: 0.9994\n",
            "Non-Temperature - avg_cores_power:\n",
            "  MSE: 0.0004\n",
            "  MAE: 0.0076\n",
            "  R²: 0.9996\n",
            "Non-Temperature - avg_routers_power:\n",
            "  MSE: 0.0019\n",
            "  MAE: 0.0159\n",
            "  R²: 0.9981\n",
            "Non-Temperature - avg_power_per_router:\n",
            "  MSE: 0.0063\n",
            "  MAE: 0.0337\n",
            "  R²: 0.9936\n",
            "Non-Temperature - layer_area:\n",
            "  MSE: 0.0001\n",
            "  MAE: 0.0036\n",
            "  R²: 0.9999\n",
            "Non-Temperature - area_per_core:\n",
            "  MSE: 0.0000\n",
            "  MAE: 0.0000\n",
            "  R²: 1.0000\n",
            "\n",
            "Combined Metrics (Temperature + Non-Temperature):\n",
            "  MSE: 0.0208\n",
            "  MAE: 0.0716\n",
            "  R²: 0.9795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Example new input data as a DataFrame (ensure it matches the structure of the features)\n",
        "new_data = pd.DataFrame({\n",
        "    'dimx': [14],\n",
        "    'dimy': [11],\n",
        "    'dimz': [2],\n",
        "    'buffer_size': [10],\n",
        "    'packet_size_min': [4],\n",
        "    'packet_size_max': [8],\n",
        "    'routing_type_oe_3d': [1],    # Make sure to align with the encoded columns\n",
        "    'selection_strategy_thermal': [1],\n",
        "    'traffic_type_random': [1],\n",
        "    'injection_rate': [0.05]\n",
        "})\n",
        "\n",
        "# Ensure all columns are present (including any missing dummy variables from the training data)\n",
        "for col in X.columns:\n",
        "    if col not in new_data:\n",
        "        new_data[col] = 0\n",
        "\n",
        "# Reorder the new_data columns to match the original feature order\n",
        "new_data = new_data[X.columns]\n",
        "\n",
        "# Scale the new data using the previously fitted scaler\n",
        "new_data_scaled = scaler_X.transform(new_data)\n",
        "\n",
        "# Predict temperature values using the FNN model\n",
        "y_temp_pred_scaled_new = fnn_model.predict(new_data_scaled)\n",
        "\n",
        "# Predict non-temperature values using the Decision Tree model\n",
        "y_non_temp_pred_scaled_new = rf_model.predict(new_data_scaled)\n",
        "\n",
        "# Inverse scale the temperature predictions\n",
        "y_temp_pred_original = scaler_y_temp.inverse_transform(y_temp_pred_scaled_new)\n",
        "\n",
        "# Inverse scale the non-temperature predictions\n",
        "y_non_temp_pred_original = scaler_y_non_temp.inverse_transform(y_non_temp_pred_scaled_new)\n",
        "\n",
        "# Combine the inverse-scaled predictions (Temperature + Non-Temperature)\n",
        "y_combined_pred_original = np.hstack((y_temp_pred_original, y_non_temp_pred_original))\n",
        "\n",
        "# Output the inverse-scaled predictions\n",
        "predictions_df = pd.DataFrame(y_combined_pred_original, columns=temperature_columns + non_temperature_columns)\n",
        "\n",
        "# Display the inverse-scaled predictions\n",
        "print(\"Predictions for the new data (original scale):\")\n",
        "print(predictions_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOkku5FsAC2C",
        "outputId": "9d03c7e4-8225-41a3-95c8-4ff44fc82c37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n",
            "Predictions for the new data (original scale):\n",
            "   steady_state_temp_L0  steady_state_temp_L1  router_avg_temp_L0  \\\n",
            "0            136.087555             134.94046           26.853331   \n",
            "\n",
            "   router_avg_temp_L1  core_avg_temp_L0  core_avg_temp_L1  mem_avg_temp_L0  \\\n",
            "0           26.575329         26.144356         25.972157        25.849583   \n",
            "\n",
            "   mem_avg_temp_L1   total_area     avg_power  avg_cores_power  \\\n",
            "0        25.714052  733857340.0  4.393888e-07     3.506394e-07   \n",
            "\n",
            "   avg_routers_power  avg_power_per_router   layer_area  area_per_core  \n",
            "0       8.874938e-08          2.881475e-10  366928660.0      4695230.0  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}